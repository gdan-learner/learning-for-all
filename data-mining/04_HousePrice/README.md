# 房屋租赁价格预测

（kaggle被谷歌收购了

赛题链接：http://challenge.xfyun.cn/topic/info?type=realtor&option=ssgy



## 1. 数据说明

赛题数据由训练集和测试集组成，总数据量超过30w，包含31个特征字段。为了保证比赛的公平性，将会从中抽取20万条作为训练集，5万条作为测试集

### 特征字段

ID、区域1、区域2、区域3、街道、上传日期、服务费、供暖费用、电力基础价格、有阳台、没有停车位、有厨房、有地窖、居住面积、房屋状况、内饰质量、可带宠物、加热类型、有电梯、房屋类型、邮政编码、房间数量、所处楼层、建筑楼层、有花园、最后翻新年份、是新建筑、建成年份、价格趋势、上传图片、房屋租金



## 2. 评估指标

评价标准采用MAE指标

![image-20220928094601152](https://gitee.com/shuangshuang853/picture-bed/raw/master/picture/20220928094602.png)

auc一般是分类，衡量分类准确率；mae是回归，衡量回归的一个差距；



预测结果文件详细说明：

\1) 以csv格式提交，编码为UTF-8，第一行为表头；

\2) 提交前请确保预测结果的格式与sample_submit.csv中的格式一致。具体格式如下：

![image-20220928094915164](https://gitee.com/shuangshuang853/picture-bed/raw/master/picture/20220928094915.png)

参数 —— eval_metric：“mae”



## 3. 统计学上的CV

标准[变异系数](https://so.csdn.net/so/search?q=变异系数&spm=1001.2101.3001.7020)是一组数据的变异指标与其平均指标之比，它是一个相对变异指标。变异系数有全距系数、平均差系数和标准差系数等。常用的是标准差系数，用CV(Coefficient of Variance)表示。

CV(Coefficient of Variance):标准差与均值的比率。

用公式表示为：CV＝σ/μ

作用：反映单位均值上的离散程度，常用在两个总体均值不等的离散程度的比较上。若两个总体的均值相等，则比较标准差系数与比较标准差是等价的。



## 4. 偏态分布

![image-20220928112822194](https://gitee.com/shuangshuang853/picture-bed/raw/master/picture/20220928112823.png)

```python
#计算col列的偏态系数
df["col"].skew()
```

如果你的数据是偏态分布的时候，这个时候可以对数据进行转换，从偏态数据转换成正态数据，常见的转换就是原始数据取对数。

```python
#对x取对数 
import math
math.log( x )
```

### 	为什么要对变量取对数？https://www.zhihu.com/question/22012482



## 5. 特征衍生

​	特征衍生也叫特征构建，是指从原始数据中构建新的特征，也属于特征选择的一种手段。

### 	5.1 特征扩展

​	基于一个特征，使用<u>特征值打平（扩展）的方式衍生多个标注类型的特征</u>，也可以理解为离散化。对于<u>分类变量，直接one-hot编码</u>；对于<u>数值型特征</u>，离散化到几个固定的区间段，然后用one-hot编码。

​	这里可以起到两个作用：

​		① 把线性函数转换成分段阶跃函数，减少过拟合。

​		② 标注 ，方便后续特征交叉组合。

### 	5.2 特征组合

​		将<u>两个或多个输入特征</u>通过数学运算进行组合，分为如下几种情况：对特征进行加，减，乘，除

### 	5.3 特征交叉

​		对<u>多个特征</u>进行交叉组合，或做交，并，补，笛卡尔集等运算

​		暴力交叉，暴力交叉可能产生稀疏问题

### 	5.4 合成特征

​		通过将<u>单独的特征</u>进行组合（相乘或求笛卡尔积）而形成的合成特征，是一种让线性模型学习到非线性特征的方式。

### 	5.5 自动衍生

​		包括自动衍生和深度衍生方法，可以缩短时间成本，构建维度更广更全面的新生特征。

​	

## 6. 特征工程

​	**特征**是从现实世界的具体物体到用数值表示的<u>抽象数值化变换</u>。

​	**特征工程**是<u>将原始数据转化为特征</u>的过程，这些特征可以<u>更好地向预测模型描述潜在问题，从而提高模型对未见数据的准确性</u>。

​	**特征工程**通常包括三个工作：特征生成、特征选择、特征编码

- **特征选择** 在许多数据分析和建模项目中，数据科学家会收集到**成百上千个特征**。更糟糕的是，有时特征数目会大于样本数目。这种情况很普遍，但在大多数情况下，并**不是所有的变量都是与机器试图理解和建模的内容相关**的。所以数据科学家可以尝试设计一些有效的方法来**选择**那些重要的特征，并将它们合并到模型中，这叫做特征选择。
- **特征生成** 一般是在特征选择之前，它提取的对象是**原始数据**，目的就是自动地构建新的特征，**将原始数据转换为一组具有明显物理意义（比如 Gabor、几何特征、纹理特征）或者统计意义的特征**。
- **特征编码** 原始数据通常比较杂乱，可能会带有各种非数字特殊符号。**而实际上机器学习模型需要的数据是数字型的，因为只有数字类型才能进行计算。**因此，对于各种特殊的特征值，我们都需要对其进行相应的编码，也是**量化**的过程。

​	在机器学习步骤中，特征工程会耗费数据科学家大量的人力去进行特征的提取和筛选，不仅耗费大量的时间，而且效率也不高。因此需要自动特征工程来将这些操作自动化，节省数据科学家的时间。



## 7. 模型

- xgboost分为gpu、cpu版：params['tree_method'] = 'gpu_hist'


- catboost多任务回归不能使用gpu


- 本地cv：指的是本地交叉验证


- lb：指的是在 Leaderboard 得到的分数，有 **Public LB** 和 **Private LB** 之分。
- xgb、lgb可以像神经网络那样读取网络继续训练。

- 高基数：高基数列是指数据基本不重复或者均为唯一值的列。典型的高基数列有ID标识，电子邮件地址或者用户名等。一个具有高基数的数据表列的例子是具有一个名为USER_ID的列的USERS表。这一列包含1-n的唯一值。每次在USERS表中创建一个新用户时，将在USER_ID列中创建一个新数字，以唯一地标识它们。由于USER_ID列中保存的值是唯一的，因此该列的基数类型被称为高基数。高基数在时序数据世界中之所以成为如此大的问题，是因为一些流行的时序数据库的局限性。



### xgboost

​	单模84.3，

​	其他的模型还有lgb和将lgb和xgb进行stacking，特征工程有两个，一个是文件里那样的，

​	一个是把每平米价格换成房屋租金的，其他没变，

​	每个模型都会用这两种特征工程，最后得到6种模型，进行加权就可以了

​	xgboost两种特征工程的加权分数在83.8。

​	但是最终方案因为lgb的权重复现不出来，所以改用autogluon，效果会差一点，但是差不了多少

​	xgb	每平米价格 全

​	lgb	每平米价格  全

​	lgb和xgb-stacking	

​	

## 8. 总结

​	绝大部分情况下lightgbm在速度和精度都比随机森林好，如果是结构化比赛一般lgb和xgb优先。sklearn中的随机森林对单机多核支持的不太行，效率很低。



## 9. 疑问



https://www.dounaite.com/article/62561931ae87fd3f795d455f.html



1. 50000条数据里面有450个数据重复？

2. autogluon 我得到的结果更差，电信的好，房价预测的差

3. stack是什么意思？

   进行stack会好一点，在95

4. 裸特征是什么意思？

5. 在使用聚类生成特征中，能不能使用target来进行聚类（target encoding）

    https://towardsdatascience.com/dealing-with-categorical-variables-by-using-target-encoder-a0f1733a4c69

    https://blog.csdn.net/lizz2276/article/details/120609596

    好像使用之后，本地cv下降了 

6. GMM算法进行label encoding，怎么让测试也能有这些特征？交叉验证  https://www.jianshu.com/p/f3dbec8a5c43

7. 糖尿病、电信(年龄和其他分组聚合，加特征)如何对特征做分组聚合？

8. 特征组合之后（两两相乘）达到2200多个特征，然后用pca降维，但是分数并没有上升，还下降了很多。

9. 类别型特征，在xgb里面要转换成自然数或one-hot。xgb不能自动处理类别特征。lgb和catboost可以。

10. 第四范式 autoX https://www.4paradigm.com/about/research.html

11. lightGBM:  https://www.bilibili.com/video/BV1ii4y1G7uf/?spm_id_from=333.337.search-card.all.click&vd_source=56672e399688ce510ed5c7b98ddfc1d7

12. 房价官方的 baseline 看代码好像没有使用tta

13. 高基数的target encoder会直接泄露标签。

14. 高基数的类别变量一般如何处理？label encoder?

15. SVM跑一百万条数据可能会很慢。

16. 信也比赛

17. 爬豆瓣短评：RPA、影刀、八爪鱼

18. 训练数据过多但特征太少解决办法 -> 提取新特征。

19. nlp标注语法树

20. xgb.feature_importances_的输出结果，和plot_importance()的输出结果不一致,显然结果不大一样。
     这是因为model.feature_importances_的重要性排名默认使用gain,而xgb.plot_importance默认使用weight。
     改一下就一样

     ![image-20221015200853269](C:/Users/96212/AppData/Roaming/Typora/typora-user-images/image-20221015200853269.png)

21. missingn模块：https://blog.csdn.net/weixin_48249563/article/details/114899818

22. 一般你用完group by  merge之类的后 需要重新设置索引 直接．reset index 然后带inplace=True这个参数就行













