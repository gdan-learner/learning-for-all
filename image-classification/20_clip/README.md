# **Learning Transferable Visual Models From Natural Language Supervision**

从**自然语言**监督中学习**可迁移**的视觉模型

用自然语言的监督信号去训练视觉模型。

**对比学习**。

# **Abstract**

![image-20221004091404059](https://gitee.com/shuangshuang853/picture-bed/raw/master/picture/20221004091405.png)

​	我们通过对超过30个不同的现有计算机视觉数据集进行基准测试来研究这种方法的性能，这些数据集跨越的任务包括<u>OCR、视频中的动作识别、地理定位和许多类型的细粒度对象分类</u>。我们在ImageNet zero-shot 上匹配原始ResNet-50的精度，而不需要使用它所训练的128万个训练示例中的任何一个。

​	开源了推理的代码，但是没有开源预训练模型。



# 引言

​	与任务不相关的目标，如自回归和蒙面语言建模，已经在计算、模型容量和数据方面扩展了许多数量级，稳步提高了能力。

​	自回归、掩码回归（完形填空）的方式，与下游任务无关。

​	采用自监督的方式反而比高质量的有标签的数据更好使用。

​	参考了 Li 2017 年 zero-shot 的论文。 

![image-20221004093205128](https://gitee.com/shuangshuang853/picture-bed/raw/master/picture/20221004093205.png)

​	

​	采用最近的架构和训练前方法，VirTex（自回归 2020）、ICMLM（完形填空 2020）和ConVIRT（于clip非常相似 但只在医疗图像上做了实验 2020）最近展示了基于transformer的语言建模、掩码语言建模和从文本中学习图像表示的对比目标的潜力。

​	科列斯尼科夫等人（2019年）和多索维茨基等人（2020年）也通过预训练模型来预测噪声标记的JFT-300M数据集的类别，在更广泛的转移基准集上取得了很大的收益。

​	 使用文本这种弱监督信号去帮助有监督的模型取得更好的效果。这些弱监督模型和最近直接从自然语言学习图像表示的探索之间的一个关键区别是尺度。Mahajan等人（2018）和科列斯尼科夫等人（2019）对数百万到数十亿张图像进行加速器年模型训练，VirTex、ICMLM和ConVIRT对1到20万张图像进行加速器日训练。

​	在这项工作中，我们缩小了这一差距，并研究了大规模的自然语言监督训练的图像分类器的行为。由大量的公开数据的这种形式在互联网上，我们创建一个新的数据集的4亿（图像、文本）对，证明一个简化版本的ConVIRT从零开始训练，我们称之为 CLIP ，对比语言图像预训练，是一个有效的学习方法从自然语言的监督。我们通过训练一系列的**8个模型**来研究CLIP的可伸缩性，并观察到传输性能是一个平滑可预测的计算函数（Hestness等人，2017；Kaplan等人，2020年）。我们发现，CLIP，类似于GPT家族，在训练前学习执行一系列广泛的任务，包括OCR、地理定位、动作识别等。

​	

![image-20221004131111677](https://gitee.com/shuangshuang853/picture-bed/raw/master/picture/20221004131112.png)

​	冻住banckbone，只训练分类头。也显示CLIP优于最好的公开使用的ImageNet模型，同时计算效率也更高。zero-shot 的CLIP模型比等效精度有监督的ImageNet模型更稳健，这表明对任务无关模型的 zero-shot 评价更能代表模型的能力。

# Approach



## **2.2. 创建足够大的数据集**

**Creating a Suffificiently Large Dataset**

​	现有的工作主要使用了三个数据集，MS-COCO（Lin等人，2014年）、 Visual Genome（Krishna等人，2017年）和YFCC100M（Thomee等人，2016年）。虽然MS-COCO和Visual Genome是高质量的人群标记数据集，但按照现代标准衡量，它们都很小，每个大约有10万张训练照片。相比之下，其他计算机视觉系统接受了多达35亿张Instagram照片的训练（Mahajan等人，2018年）。YFCC100M，有1亿张照片，是一个可能的替代方案，但质量不好。许多图像使用自动生成的文件名，如20160716 113957.JPG作为“标题”或包含相机曝光设置的“描述”。经过过滤，只保留具有自然语言标题或英文描述的图像后，数据集缩小了6倍，缩小到只有1500万张照片。这与ImageNet的大小大致相同。因此作者自己去造一个大的数据集，不光是孕育了CLIP这篇论文的工作，而且还孕育了DALL`E这篇图像生成的工作。

​	我们构建了一个新的数据集，其中包括4亿对（图像，文本），这些数据集从互联网上的各种公开来源中收集起来。所得到的数据集与用于训练GPT-2的WebText数据集具有相似的总字数。我们将这个数据集称为WIT，也就是WebImageText。



## 2.3 选择预训练方法

**Selecting an Effificient Pre-Training Method**

​	从相同的单词袋编码基线开始，我们将预测目标替换为图2中的一个对比目标，并观察到转移到ImageNet的零镜头转移率的效率进一步提高了4倍。

​	![image-20221004161635404](https://gitee.com/shuangshuang853/picture-bed/raw/master/picture/20221004161636.png)

图2. CLIP在 **zero-shot** 传输时比我们的图像标题基线更有效。虽然高度表达，但我们发现 基于transformer 的语言模型在 **zero-shot** ImageNet分类时相对较弱。在这里，我们看到它的学习速度<u>比预测文本的单词包</u>（BoW）编码的基线慢3倍（Joulin et al.，2016）。将预测目标转换为CLIP的对比目标，进一步提高效率4倍。



### 伪代码

![image-20221004163616751](https://gitee.com/shuangshuang853/picture-bed/raw/master/picture/20221004163617.png)



1. 不太容易过拟合。
2. 不适用非线性投射层，使用了线性投射层。
3. 只是用了随即裁剪的数据增强。



## 2.5 训练 **Training**

​	我们训练了5个resnet和3个vit 。对于ResNets，我们训练一个ResNet-50，一个ResNet-101，然后再训练3个，它们遵循EfficientNet模型缩放，使用大约ResNet-50计算的4倍、16倍和64倍。它们分别记为RN50x4、RN50x16和RN50x64。对于vit，我们训练了ViT-B/32、ViT-B/16和ViT-L/14 （32、16、14分别指的是patch的大小）。我们训练了所有的模型32个epoch。 我们使用Adam优化器去训练，与解耦的权重衰减正则化，应用于所有非增益或偏差的权重，并使用余弦调度衰减学习率。初始超参数是在训练**1个epoch**时，使用 <u>grid searches 、random search</u>和对基线<u>ResNet- 50</u> 模型进行手动调整的组合来设置的。

Batch-size 设置成32,768. 混合精度用于加速训练和节省内存。

最大的ResNet型号：RN50x64，在592个V100gpu上训练花了18天，而最大的 vit 在256个V100gpu上训练花了12天。对于ViT-L/14，我们还以更高的336像素分辨率进行了一个额外的epoch的预训练（fine-tune），以提高类似于FixRes的性能（Touvron等人，2019年）。我们将这个模型表示为**ViT-L/14@336px**。除非另有说明，本文中报告的“CLIP”的所有结果都使用这个我们认为表现最好的模型。

# 3 实验

## 3.1  zero-shot 迁移学习 

**Zero-Shot Transfer**

​	虽然在无监督学习领域的许多研究都集中在机器学习系统的表征学习能力上，但我们激励着研究zero-shot 转移，作为衡量机器学习系统的任务学习能力的一种方式。

​	

![image-20221004194925617](https://gitee.com/shuangshuang853/picture-bed/raw/master/picture/20221004194926.png)

​	在表1中，我们比较了 Visual N-Grams 。最好的CLIP模型将ImageNet上的精度从11.5%提高到76.2%，并与原始的ResNet-50的性能相匹配，<u>尽管没有使用该数据集的128万个人群标记的训练示例</u>。此外，CLIP模型的top-5准确率明显高于top-1，该模型的top-5准确率为95%，与《盗梦空间-v4》相匹配（Szegedy et al.，2016）。在零射击设置下，匹配强大的、完全监督的基线的能力表明

​	我们在一个大10倍的数据集上进行训练，使用一个视觉模型，每次预测需要近100倍的计算，可能使用超过1000倍的训练计算，并使用一个基于transformer的模型，这在 Visual N-Grams 发布时Transformer并没出现。

​	

### 3.1.4. PROMPT ENGINEERING AND ENSEMBLING

​	基于prompt的方向无论是基于nlp还是cv方向，都非常火。

​	在做微调或者是在做推理的时候用的方法，而不是在预训练阶段，所以不需要这么多的计算资源。但是效果好，所以影响力非常大。

为什么要做 prompt engineering 和 prompt ensembling?

1. 一个常见的问题是多义主义。

   如果只把一个单词去做prompt的话，会经常出现歧义性的问题。

   当一个类的名称是提供给CLIP的文本编码器的一个单词时，由于缺乏上下文，它无法区分这意味着哪个词的意义。在某些情况下，同一个单词的多个含义可能会作为不同的类包含在同一个数据集中！这发生在ImageNet中，它包括建筑起重机和飞行的起重机。另一个例子是在Oxford-IIIT宠物数据集的类别中，从上下文中，显然来说指的是一种狗，但缺乏上下文的文本编码器同样可能指的是一种运动员。

2.  我们遇到的另一个问题是，在我们的训练前的数据集中，与图像配对的文本只是一个单词是相对罕见的。通常文本是一个完整的句子，以某种方式描述图像。为了帮助弥合这种分布差距，我们使用**提示模板** “A photo of a *{*label*}*.” 。仅使用此提示符就可将ImageNet上的准确率提高1.3%。

在Oxford-IIIT宠物上，使用“一张{标签}的照片，一种宠物。”为了帮助提供上下文，工作效果很好。同样，在Food 101上指定一种食物，在FGVC飞机上一种飞机也有帮助。对于OCR数据集，我们发现在需要识别的文本或数字周围添加引号可以提高性能。

我们还尝试了集成多个prompt ensembling，所用几个提示的模板。做多次推理，然后把结果综合起来，emsemble一般都会有更好的结果。



![image-20221004202757471](https://gitee.com/shuangshuang853/picture-bed/raw/master/picture/20221004202758.png)

​	Linear Probe ：把预训练好的模型冻住，只从里面抽特征，就训练最后一层fc分类头层。去做有监督的分类任务。

​	然而，我们需要注意的是，对于特别难的任务，使用zero-shot不是特别合理，使用**few-shot**的迁移可能会更合理，比如给人类的淋巴结肿瘤分类（可能需要特定种类知识的任务，对于人来说，没有这种先验知识，我们也没法分类正确）。

​	这里的few-shot是指将模型参数冻住，只从里面去抽取特征做这种linear probe（训练最后的分类头，所以需要下游数据集里面有标签的数据），

![image-20221004210523641](https://gitee.com/shuangshuang853/picture-bed/raw/master/picture/20221004210524.png)

图6. **Zero-shot** CLIP的性能优于 **few-shot** 线性探头。Zero-shot CLIP与在相同特征空间上训练的4次线性分类器的平均性能相匹配，并且几乎在公开模型中与16次线性分类器的最佳结果相匹配。对于BiT-M和SimCLRv2，都突出显示了性能最好的模型。浅灰色的线条是eval套件中的其他模型。在此分析中使用了每个类至少有16个例子的20个数据集。



## 3.2  特征学习

​	如果下游数据全部使用，CLIP的效果会如何？

​	 先预训练一个模型，然后在下游任务上用全部的数据做微调，所以就可以在特征学习的方式做公平对比。如果下游数据用全部的数据，有很多的方式去衡量学到的特征好不好，一种是 Linear Probe ，另一种方式是 fine-tune（把网络放开，直接做端到端的学习，微调往往比linear-probe的效果更好）。

​	但是本文用的是linear probe。能更准确模型的好坏。



# 6. 局限

1. 大规模

2. zero-shot 在有些细分类的数据集上表现并不好，clip的效果低于有监督训练（res50基线）。

3. 无法处理更抽象/更难的任务。数一数有多少个物体，当前视频里是异常还是非异常

4. 如果推理的数据与训练的数据分布差的很大的话，照样差的很大。

5. CLIP也没有解决深度学习的数据效率较低的问题。

6. 做了很多次测试推理，定下来一套参数结构。（27个数据集不一定具有代表性）

7. 数据没有经过清洗过。可能有偏见（肤色、宗教）

8. 对于很复杂的概念，提供一些训练样本可能会更好。但奇怪的是，有时候提供 one-shot、two-shot，结果可能更比不上zero-shot。

   怎样能够在zero-shot的时候做的很好，同时，也能在给一些训练样本的时候，few-shot也做的很好。

一个值得尝试的简单想法是联合训练一个对比和生成的目标，并希望将CLIP的效率与标题模型的灵活性相结合。

​	

# 9 结论



![image-20221005083446439](https://gitee.com/shuangshuang853/picture-bed/raw/master/picture/20221005083447.png)













