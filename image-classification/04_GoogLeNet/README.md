# **Going deeper with convolutions**

发表于2014，与VGG是同一年提出

# 作者



![image-20220817110334206](https://gitee.com/shuangshuang853/picture-bed/raw/master/image-20220817110334206.png)

​	GoogLeNet在2014年由Google团队提出，斩获当年lmageNet竞赛中Classification Task(分类任务)第一名。

网络中的亮点:

- 引入了Inception结构（融合不同尺度的特征信息)
- 使用1x1的卷积核进行降维以及映射处理
- 添加两个辅助分类器帮助训练（AlexNet和VGG都只有一个输出层，GoogLeNet有三个输出层(其中两个辅助分类层)）
- 丢弃全连接层，使用平均池化层(大大减少模型参数)

# 摘要

​	我们提出了一种代号为“盗梦空间”的深度卷积神经网络架构，它负责在2014年ImageNet大规模视觉识别挑战(ILSVRC14)中设置分类和检测的新技术状态。该体系结构的主要特点是**提高了网络内部计算资源的利用率**，实现方法是当<u>保持计算耗费稳定时让此结构的微妙设计使得网络的深度和宽度都增加</u>。为了优化质量，体系结构的决策是基于赫布原理和多尺度处理的直觉。在我们提交ILSVRC14时使用的一个特殊化身是GoogLeNet，这是一个**22层**的深度网络，其质量是在分类和检测的背景下进行评估的。



# 5 结论

​	在这项工作中，我们评估了非常深的卷积网络（多达19个权重层）的大尺度图像分类。结果表明，表示深度有利于分类精度，并且可以使用传统的ConvNet挑战数据集上实现最先进的性能(LeCun等人，1989；克里日夫斯基等人，2012)，大幅增加深度。在附录中，我们还展示了我们的模型可以很好地推广到广泛的任务和数据集，匹配或优于围绕较不深的图像表示构建的更复杂的识别管道。我们的研究结果再次证实了深度在视觉表征中的重要性。



# 1 引言

​	在过去的三年里，主要由于深度学习的进步，更具体地说是卷积网络[10]，图像识别和目标检测的质量一直在以惊人的速度进步。一个令人鼓舞的消息是，这些进展不仅仅是更强大的硬件、更大的数据集和更大的模型的结果，而且主要是新想法、算法和改进的网络架构的结果。例如，除了ILSVRC 2014竞赛的分类数据集外，没有使用新的数据源。我们提交给ILSVRC 2014的谷歌网络实际上比克里热夫斯基等人[9]两年前的获奖架构少了12个×的参数，同时也更准确得多。目标检测方面最大的收益不是来自使用深度网络或更大的模型，而是来自深度架构和经典计算机视觉的协同作用，比如Girshick等[6]的R-CNN算法。

​	另一个值得注意的因素是，随着移动计算和嵌入式计算的持续发展，我们的算法的效率——特别是它们的能力和内存使用——变得越来越重要。值得注意的是，导致本文中提出的深层架构设计的考虑包括了这个因素，而不是完全固定在精度数字上。对于大多数的实验，模型设计保持15亿的计算预算增加亿在推理时间，所以他们不会成为一个纯粹的学术好奇心，但可以把现实世界使用，即使在大型数据集，在一个合理的成本。

​	在本文中，我们将重点研究一种高效的计算机视觉深度神经网络架构，代号为《盗梦空间》，它得名于Lin等[12]在网络论文中发表的网络和著名的“我们需要更深入”的网络模因[1]。在我们的例子中，“deep”一词有两种不同的含义：首先，我们以“盗梦空间模块”的形式引入了一个新的组织层次，也在更直接的意义上增加了网络深度。一般来说，我们可以从Arora等[2]的理论工作中获得[12]的灵感和指导。该架构的好处在ILSVRC 2014的分类和检测挑战上进行了实验验证，其中它的性能显著优于当前的最新技术水平。



# 2 相关工作

​	从LeNet-5[10]开始，卷积神经网络(CNN)通常有一个标准的结构——堆叠卷积层（可以是对比归一化和最大池），然后是一个或多个完全连接的层。这种基本设计的变体在图像分类文献中普遍存在，并在MNIST、CIFAR和最显著的ImageNet分类挑战[9,21]上取得了最好的结果。对于更大的数据集，如Imagenet，最近的趋势是增加层数[12]和层大小[21,14]，同时使用dropout[7]来解决过拟合的问题。

​	尽管担心最大池化层会导致准确空间信息的丢失，但与[9]相同的卷积网络架构也已成功地用于定位[9,14]、目标检测[6,14,18,5]和人体姿态估计[19]。受灵长类动物视觉皮层的神经科学模型的启发，Serre等人[15]使用一系列不同大小的固定Gabor过滤器来处理多个尺度，类似于盗梦空间模型。然而，与固定的固定2层深度模型相反，初始模型中的所有滤波器都是学习的。此外，初始层重复多次，在GoogLeNet模型中形成了22层深度模型。

​	Network-in-Network 是Lin等人[12]提出的一种提高神经网络表征能力的方法。当应用于卷积层时，该方法可以看作是额外的1×1个卷积层，然后通常是校正的线性激活[9]。这使得它能够很容易地集成到当前的CNN管道中。我们在体系结构中大量使用这种方法。然而，在我们的设置中，1×1卷积有双重目的：最重要的是，它们主要被用作降维模块，以消除计算瓶颈，否则这将限制我们的网络的大小。这不仅允许增加深度，还允许增加我们的网络的宽度，而不会造成显著的性能损失。

​	Girshick等人[6]提出的具有卷积神经网络的区域(R-CNN)。R-CNN将整个检测问题分解为两个子问题：首先以类别无关的方式利用潜在的对象建议的颜色和超像素一致性，然后使用CNN分类器来识别这些位置的对象类别。这种两阶段的方法利用了具有低水平线索的边界盒分割的准确性，以及最先进的cnn的高度强大的分类能力。我们在检测提交中采用了类似的管道，但在这两个阶段都探索了增强，如用于更高对象边界框召回的多盒[5]预测，以及用于更好地对边界框建议进行分类的集成方法。



# 3 **Motivation and High Level Considerations**

​	提高深度神经网络性能的最直接的方法是增加它们的规模。这包括增加网络的深度-级别的数量和它的宽度：每个级别的单位数量。这是一种训练更高质量模型的简单和安全的方法，特别是考虑到大量标记训练数据的可用性。然而，这个简单的解决方案也有两个主要的缺点。

​	更大的尺寸通常意味着大量的参数，这使得扩大后的网络更容易发生过拟合，特别是在训练集中标记的例子数量有限的情况下。这可能成为一个主要的瓶颈，因为创建高质量的训练集可能是很棘手的而且价格昂贵，特别是如果需要专家的人工评分员来区分细粒度的视觉类别，如ImageNet(即使是在1000类ILSVRC子集中)，如图1所示。

![image-20220817112111436](https://gitee.com/shuangshuang853/picture-bed/raw/master/image-20220817112111436.png)

​	网络规模的另一个缺点是计算资源的使用显著增加。例如，在深度视觉网络中，如果两个卷积层被链接起来，它们的滤波器数量的任何均匀增加都会导致计算量的二次增加。如果增加的容量使用效率较低（例如，如果大多数权重最终接近于零），那么就会浪费大量的计算。由于在实践中，计算预算总是有限的，因此即使其主要目标是提高结果的质量，也更有利于计算资源的有效分配。

​	解决这两个问题的基本方法将是最终从完全连接到稀疏连接的架构，甚至在卷积内部。除了模拟生物系统外，由于Arora等人的[2]的开创性工作，这也将具有更坚实的理论基础的优势。他们的主要结果表明，如果数据集的概率分布是由一个大型的，非常稀疏的深度神经网络，那么最优网络拓扑可以构造一层通过分析的相关统计的激活最后一层和聚类神经元与高度相关的输出。虽然严格的数学证明需要非常强的条件，但这一说法与众所周知的赫比原理——神经元在一起，在一起——表明，在实践中，即使在不那么严格的条件下，基本思想也适用。

​	缺点是，今天的计算基础设施在涉及到非均匀稀疏数据结构的数值计算方面非常低效。即使算术运算的数量减少了100×，查找和缓存丢失的开销是如此的主导，以至于切换到稀疏矩阵也不会得到回报。通过使用稳步改进、高度调优的数字库，允许极其快速的密集矩阵乘法，利用底层CPU或GPU硬件[16,9]的微小细节，进一步扩大了差距。此外，非均匀稀疏模型需要更复杂的工程和计算基础设施。目前大多数面向视觉的机器学习系统只是利用卷积来利用空间领域的稀疏性。然而，卷积是实现为到早期层中补丁的密集连接的集合。自[11]以来，ConvNets传统上在特征维度上使用随机和稀疏连接表，为了打破对称性和提高学习能力，为了更好地优化并行计算，趋势重新转向与[9]的完全连接。结构的均匀性和大量的过滤器和更大的批量大小允许利用有效的密集计算。

​	这就提出了一个问题，是否还有下一步中间步骤的希望：一个利用额外稀疏性的架构，即使是在过滤器级别，就像该理论所建议的那样，但利用了我们的目前的硬件是利用在密集矩阵上的计算。关于稀疏矩阵计算（如[3]）的大量文献表明，将稀疏矩阵聚类成相对密集的子矩阵，往往会给出稀疏矩阵乘法的最先进的实际性能。认为在不久的将来，类似的方法将被用于自动构建非统一的深度学习架构，这似乎并不牵强。

​	《初始》架构最初是第一作者的一个案例研究，用于评估复杂网络拓扑构造算法的假设输出，该算法试图近似[2]对视觉网络隐含的稀疏结构，并通过密集的、随时可用的组件覆盖假设的结果。尽管这是一项高度推测性的工作，但只有在对拓扑的精确选择进行了两次迭代之后，我们就已经可以看到对基于[12]的参考体系结构的适度收益。在进一步调整了学习速率、超参数和改进的训练方法后，我们确定了生成的初始空间架构作为[6]和[5]的基础网络，在定位和目标检测中特别有用。有趣的是，虽然大多数原始的架构选择都受到了彻底的质疑和测试，但它们至少在本地是最优的。

​	架构已经在计算机视觉领域取得了成功，但它的质量是否可以归因于导致其构建的指导原则仍然值得怀疑。确保需要更彻底的分析和验证：例如，如果基于下面描述的原则的自动化工具能够为视觉网络找到类似但更好的拓扑结构。最令人信服的证明是，如果一个自动化系统能够创建网络拓扑，使用相同的算法，在其他领域产生类似的收益。至少，《盗梦空间》架构的最初成功为这个方向的未来工作带来了坚实的动力。



# 4 Inception 架构细节

​	初始架构的主要思想是基于如何发现卷积视觉网络中的最优局部稀疏结构可以被现成的密集组件近似和覆盖。注意，假设平移不变性意味着我们的网络将由卷积构建块构建。我们所需要的就是找到最优的局部构造，并在空间上重复它。Arora等人[2]提出了一种逐层的构造，其中应该分析最后一层的相关性统计数据，并将其聚类为具有高相关性的单元组。这些集群构成了下一层的单元，并连接到前一层中的单元。我们假设前一层的每个单元对应于输入图像的某个区域，这些单元被分组为滤波器组。在较低的层（靠近输入的层）中，相关的单元将集中在局部区域。这意味着，我们最终会有许多集群集中在一个区域，它们可以在下一层被1×1卷积覆盖，正如[12]所建议的那样。然而，我们也可以预期，将会有更少的空间上更分散的集群，可以被更大的补丁上的卷积覆盖，而在更大和更大的区域上，补丁的数量将会减少。为了避免补丁对齐问题，初始架构的当前版本被限制在过滤器大小为1×1,3×3和5×5，但是这个决定更多的是基于方便而不是必要。这也意味着，建议的架构是所有这些层的组合，它们的输出滤波器组连接成一个单一的输出向量，形成下一阶段的输入。此外，由于池化操作对于在当前最先进的卷积网络中取得成功至关重要，因此它建议在每个这样的阶段添加一个替代的并行池化路径也应该产生额外的有益效果(见图2(a))。

​	这些“初始模块”堆叠，他们的输出相关统计必然会不同：特性更高的抽象被更高的层，他们的空间浓度将减少表明3×3和5×5卷积应该增加我们搬到更高的层。

​	上述模块的一个大问题，至少在这种形式下是，即使是在具有大量滤波器的卷积层上，即使是少量的5×5卷积也可能非常昂贵。一旦将池化单元添加到混合单元中，这个问题就会变得更加明显：它们的输出过滤器的数量等于上一阶段的过滤器的数量。池化层的输出与卷积层的输出的合并将不可避免地导致一个各阶段间产出数量的增加。即使这种体系结构可能覆盖最优稀疏结构，但它也会做到效率很低，在几个阶段内导致计算爆炸。

![image-20220817113629996](https://gitee.com/shuangshuang853/picture-bed/raw/master/image-20220817113629996.png)



### 为什么要使用 1×1 的卷积？

![image-20220817150856838](https://gitee.com/shuangshuang853/picture-bed/raw/master/image-20220817150856838.png)	通过计算可以看出能够减少参数。

这就导致了所提出的架构的第二个想法：在其他计算需求会增加过多的地方，明智地应用减维和投影。这是基于嵌入的成功：即使是低维的嵌入也可能包含大量关于一个相对较大的图像补丁的信息。然而，嵌入以密集的、压缩的形式表示信息，而压缩的信息更难建模。我们希望在大多数地方保持我们的表示稀疏（根据[2]的条件的要求），并且只在信号必须集体聚集时压缩它们。也就是说，在昂贵的3×3和5×5卷积之前，1×1卷积用于计算缩减。除了被用作减少量外，它们还包括使用校正的线性激活，这使它们具有双重用途。最终的结果如图2(b).所示

​	一般来说，初始网络是由上述类型的模块堆叠组成的网络，偶尔有最大池层，步幅为2，使网格分辨率减半。出于技术原因（训练期间的记忆效率），只在较高的层开始使用初始空间模块，同时保持传统的卷积方式似乎是有益的。这并不是严格必要的，只是反映了我们目前实施中的一些基础设施低低下。

​	这种体系结构的一个主要好处方面是，它允许在每个阶段显著增加单元的数量，而不会出现不受控制的计算复杂性膨胀。降维的普遍使用允许将最后阶段的大量输入滤波器屏蔽到下一层，首先减少它们的维数，然后与大的补丁大小进行卷积。这种设计的另一个实际有用的方面是，它与直觉相一致，即视觉信息应该在不同的尺度上进行处理，然后进行聚合，以便下一阶段可以同时从不同的尺度上提取特征。

​	计算资源的改进使用允许增加每个阶段的宽度和阶段的数量，而不涉及计算困难。利用初始架构的另一种方法是创建质量稍低，但计算成本较低的版本。我们发现，所有包含的旋钮和杠杆都允许计算资源的控制平衡，从而导致网络比非初始架构的网络快2−3×，但在这一点上需要仔细的手动设计。



# 5 GoogLeNet

​	我们在ILSVRC14比赛中选择了GoogLeNet作为我们的团队名称。这个名字是对Yann LeCuns的先驱LeNet 5网络[10]的致敬。我们还使用GoogLeNet来引用我们在提交的比赛中使用的初始架构的特定化身。我们还使用了一个更深层次、更广泛的初始网络，它的质量略差，但将其添加到集成中似乎略微提高了结果。我们省略了该网络的细节，因为我们的实验已经表明，确切的架构参数的影响是相对的较小的这里，表1描述了最成功的特定实例(名为GoogLeNet)。在我们的集成中，7个模型中有6个使用了完全相同的拓扑结构（用不同的采样方法训练）。

![image-20220817114217506](https://gitee.com/shuangshuang853/picture-bed/raw/master/image-20220817114217506.png)

​	

![image-20220817152627107](https://gitee.com/shuangshuang853/picture-bed/raw/master/image-20220817152627107.png)

​	

​	所有的卷积，包括那些在初始模块内部的卷积，都使用了修正的线性激活。在我们的网络中，接受野的大小是224×224，采用RGB颜色通道的平均减法。“#3×3减少”和“#5×5减少”表示在3×3和5×5卷积之前的还原层中使用的1×1滤波器的数量。在池proj列中内置的最大池化之后，我们可以在投影层中看到1个×1过滤器的数量。所有这些还原/投影层也使用了校正的线性激活。

​	该网络的设计考虑到了计算效率和实用性，因此推理可以在单个设备上运行，甚至包括那些计算资源有限的设备，特别是低内存占用的设备。当只计算带有参数的层时，该网络有22层深（如果我们也计算池化，则是27层）。用于建设网络的层（独立构件）约为100个。然而，这个数字取决于所使用的机器学习基础设施系统。在分类器之前使用平均池化是基于[12]的，尽管我们的实现的不同之处在于我们使用了一个额外的线性层。这使得我们的网络可以很容易地适应和微调其他标签集，但它主要是方便的，我们不期望它会产生重大影响。研究发现，从完全连接层到平均池化的前1位精度提高了约0.6%，然而，即使去除完全连接层，使用dropout仍然是必要的。

​	现表明，在网络中间的层所产生的特征应该是非常有区别的。通过添加连接到这些中间层的辅助分类器，我们将期望在分类器的较低阶段鼓励区分，增加传播回的梯度信号，并提供额外的正则化。这些分类器采用了较小的卷积网络的形式，并将它们**放在初始空间(4a)和(4d)模块的输出之上**。在训练过程中，它们的损失以折扣权值加到网络的总损失中（辅助分类器的损失加权为0.3）。在推理时，这些辅助网络被丢弃。

<img src="https://gitee.com/shuangshuang853/picture-bed/raw/master/image-20220817145443631.png" alt="image-20220817145443631" style="zoom:80%;" />



包括辅助分类器在内的侧面额外网络的确切结构如下（上图绿色框）：

- 平均池化层具有5×5过滤器大小和步幅3，导致(4a)输出<u>4×4×512</u>，(4d)阶段输出<u>4×4×528</u>。
- 一个 1×1卷积与128个滤波器，用于降维和ReLU激活函数。
- 一个具有1024个单元和ReLU激活函数的全连接层。
- 70%的dropout层，随机失活神经元
- 以softmax损失作为分类器的线性层（预测与主分类器相同的1000个类，**但在推理时删除**）

![image-20220817153221340](https://gitee.com/shuangshuang853/picture-bed/raw/master/image-20220817153221340.png)

结果网络的示意图如图3所示。





# 6 训练方法

​	我们的网络使用干扰信念[4]分布式机器学习系统进行训练，使用少量的模型和数据并行性。虽然我们只使用了基于CPU的实现，但一个粗略的估计表明，GoogLeNet网络可以在一周内使用少量的高端gpu来训练到收敛，主要的限制是内存的使用。我们的训练使用了具有0.9动量[17]的异步随机梯度下降，固定的学习速率计划（每8个周期将学习速率降低4%）。使用Polyak平均[13]来创建在推理时使用的最终模型。

​	我们的图像采样方法已经大大改变了几个月导致竞争，和已经收敛模型训练与其他选项，有时结合改变超参数，如辍学和学习率，所以很难给一个明确的指导最有效的单一方法来训练这些网络。更复杂的是，一些模型主要在较小的相对作物上训练，另一些在较大的作物上，受[8]的启发。尽管如此，在比赛后被验证非常有效的处方包括对不同大小的图像斑块进行采样，其大小均匀分布在图像面积的8%到100%之间，其高宽比在3/4到4/3之间随机选择。此外，我们还发现Andrew Howard[8]的光度畸变在一定程度上有助于对抗过拟合。此外，我们开始使用随机插值方法（双线性、面积、最近邻和三次，等概率）来调整相对较晚的大小，并结合其他超参数变化，因此我们不能确定最终结果是否受到它们的积极影响。



# 7 ILSVRC 2014分类挑战设置和结果

​	ILSVRC 2014年的分类挑战涉及到将图像分类为图像集层次结构中的1000个叶节点类别中的一个。大约有120万张图像用于训练，5万张用于验证，10万张图像用于测试。每个图像与一个地面真实类别相关联，性能测量基于最高得分分类器预测。两个数字通常报道：前1准确率，比较地面真相与第一个预测类，和前5错误率，比较地面真相与前5个预测类：一个图像被认为是正确分类如果地面真相在前5，不管其排名。该挑战使用了排名前5名的错误率来进行排名。我们参与了这个挑战，没有使用任何用于培训的外部数据。除了本文中提到的训练技术外，我们在测试过程中还采用了一组技术来获得更高的性能，我们将在下面详细阐述。

1. 我们独立训练了同一个GoogLeNet模型的7个版本（包括一个更广泛的版本），并使用它们进行了集成预测。这些模型使用相同的初始化（即使使用相同的初始权重，主要是由于疏忽）和学习率策略进行训练，它们只在采样方法和它们看到输入图像的随机顺序上有所不同。
2. 在测试过程中，我们采用了比克里日夫斯基等[9]更激进的种植方法。具体来说，我们将图像的大小调整为4个尺度，其中较短的维度（高度或宽度）分别为256、288、320和352，取这些调整大小的图像的左、中和右的正方形（在肖像图像的情况下，我们取上、中和下的正方形）。对于每个正方形，我们取4个角和中心224×224作物以及正方形的大小调整到224×224，以及它们的镜像版本。结果每张图像有4×3×6×2=144个作物。Andrew Howard[8]在前一年的记录中使用了类似的方法，我们通过经验验证了它的表现比提议的方案略差。我们注意到，这种激进的种植在实际应用中可能没有必要，因为在合理数量的作物出现后，更多作物的效益变得边际（我们将在后面展示）。
3. 在多个作物和所有单个分类器上对softmax概率进行平均，以获得最终的预测。在我们的实验中，我们分析了验证数据的替代方法，如对作物的最大汇集和对分类器进行平均，但它们导致的性能低于简单的平均。

​	在本文的其余部分中，我们分析了有助于最终提交的整体性能的多个因素。

​	我们在挑战中的最终提交在验证和测试数据上都获得了6.67%的前5名错误，在其他参与者中排名第一。与2012年的超人方法相比，这是一个相对减少了56.5%，与前一年的最佳方法(Clarifai)相比，这两个相对减少了约40%，这两种方法都使用外部数据来训练分类器。下表显示了一些性能最好的方法的统计数据。

​	我们还分析和报告了多个测试选择的性能，通过改变模型的数量和作物的数量来预测一个图像。当我们使用一个模型时，我们选择了验证数据上前1位错误率最低的一个模型。所有的数字都报告在验证数据集上，以不过度适合测试数据的统计数据。



# 8 ILSVRC 2014检测挑战设置和结果

​	ILSVRC的检测任务是在200个可能的类中，围绕着图像中的对象生成边界框。如果检测到的对象与地面真相的类匹配，并且它们的边界框至少重叠了50%(使用Jaccard索引)，则索数正确。外来检测被视为假阳性并受到惩罚。与分类任务相反，每个图像都可能包含许多物体或没有，它们的规模可能从大到小不等。使用平均平均精度(mAP)报告结果。

​	GoogLeNet所采用的检测方法类似于[6]的R-CNN，但增加了初始模型作为区域分类器。此外，通过结合选择性搜索[20]方法与多盒[5]预测，以提高更高的目标边界框召回率，改进了区域建议步骤。为了减少假阳性的数量，超像素的大小增加了2×。这使得选择性搜索算法的建议的一半。我们重新添加了200个来自多盒[5]的区域提案，总共约占[6]使用的提案的60%，同时将覆盖率从92%提高到93%。减少数量和增加覆盖率的提案数量的总体效果是，对单个模型情况的平均平均精度提高了1%。最后，在对每个区域进行分类时，我们使用了6个convnet的集合，将结果的准确率从40%提高到43.9%。请注意，与R-CNN相反，由于缺乏时间，我们没有使用边界盒回归。

​	我们首先报告顶级检测结果，并显示自第一版检测任务以来的进展。与2013年的调查结果相比，准确率几乎翻了一番。表现最好的团队都使用卷积网络。我们在表4中报告了官方分数和每个团队的常见策略：外部数据的使用、集成模型或上下文模型。外部数据通常是ILSVRC12分类数据，用于预训练一个模型，然后对检测数据进行细化。一些团队还提到了本地化数据的使用。由于大部分定位任务边界框不包含在检测数据集中，因此可以用这些数据预训练通用边界框回归器，分类方法与预训练相同。GoogLeNet条目没有使用本地化数据进行预训练。

​	在表5中，我们只使用单个模型来比较结果。表现最好的模型是通过深度洞察，令人惊讶的是，通过3个模型的集成只提高了0.3分，而GoogLeNet通过集成获得了显著更强的结果。

![image-20220817143018949](https://gitee.com/shuangshuang853/picture-bed/raw/master/image-20220817143018949.png)



# 9 结论

​	我们的结果似乎产生了一个坚实的证据，即通过现成的密集构建块来近似预期的最优稀疏结构是改进计算机视觉神经网络的可行方法。这种方法的主要优点是与较浅和较不宽的网络相比，在计算需求的适度增加下显著提高质量。还要注意，我们的检测工作具有竞争力，尽管我们既没有利用上下文，也没有执行边界框回归和这一事实为初始架构的强度提供了进一步的证据。虽然期望通过更昂贵的相似深度和宽度的网络可以实现相似的结果质量，但我们的方法提供了坚实的证据，证明转向稀疏架构是可行和有用的想法。这表明未来将在[2]的基础上以自动化的方式创建更稀疏和更精细的结构。





