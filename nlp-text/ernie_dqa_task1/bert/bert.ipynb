{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 1.Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset squad (C:/Users/96212/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78d852399ac84fcbad8224fd69049edd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# squad_v2等于True或者False分别代表使用SQUAD v1 或者 SQUAD v2。\n",
    "# 如果您使用的是其他数据集，那么True代表的是：模型可以回答“不可回答”问题，也就是部分问题不给出答案，而False则代表所有问题必须回答。\n",
    "squad_v2 = False\n",
    "# 下载数据（确保有网络）\n",
    "datasets = load_dataset(\"squad_v2\" if squad_v2 else \"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '5733be284776f41900661182',\n",
       " 'title': 'University_of_Notre_Dame',\n",
       " 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
       " 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',\n",
       " 'answers': {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datasets import ClassLabel, Sequence\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "\n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "        elif isinstance(typ, Sequence) and isinstance(typ.feature, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda x: [typ.feature.names[i] for i in x])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5725ce3c89a1e219009abf03</td>\n",
       "      <td>Montevideo</td>\n",
       "      <td>Also of major note in Ciudad Vieja is the Plaza de la Constitución (or Plaza Matriz). During the first decades of Uruguayan independence this square was the main hub of city life. On the square are the Cabildo—the seat of colonial government—and the Montevideo Metropolitan Cathedral. The cathedral is the burial place of Fructuoso Rivera, Juan Antonio Lavalleja and Venancio Flores. Another notable square is Plaza Zabala with the equestrian statue of Bruno Mauricio de Zabala. On its south side, Palacio Taranco, once residence of the Ortiz Taranco brothers, is now the Museum of Decorative Arts. A few blocks northwest of Plaza Zabala is the Mercado del Puerto, another major tourist destination.</td>\n",
       "      <td>What is the Cabildo?</td>\n",
       "      <td>{'text': ['the seat of colonial government'], 'answer_start': [210]}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(datasets[\"train\"], num_examples=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 2. data process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 2000,\n",
       " 3183,\n",
       " 2106,\n",
       " 1996,\n",
       " 6261,\n",
       " 2984,\n",
       " 9382,\n",
       " 3711,\n",
       " 1999,\n",
       " 8517,\n",
       " 1999,\n",
       " 10223,\n",
       " 26371,\n",
       " 2605,\n",
       " 1029,\n",
       " 102,\n",
       " 6549,\n",
       " 2135,\n",
       " 1010,\n",
       " 1996,\n",
       " 2082,\n",
       " 2038,\n",
       " 1037,\n",
       " 3234,\n",
       " 2839,\n",
       " 1012,\n",
       " 10234,\n",
       " 1996,\n",
       " 2364,\n",
       " 2311,\n",
       " 1005,\n",
       " 1055,\n",
       " 2751,\n",
       " 8514,\n",
       " 2003,\n",
       " 1037,\n",
       " 3585,\n",
       " 6231,\n",
       " 1997,\n",
       " 1996,\n",
       " 6261,\n",
       " 2984,\n",
       " 1012,\n",
       " 3202,\n",
       " 1999,\n",
       " 2392,\n",
       " 1997,\n",
       " 1996,\n",
       " 2364,\n",
       " 2311,\n",
       " 1998,\n",
       " 5307,\n",
       " 2009,\n",
       " 1010,\n",
       " 2003,\n",
       " 1037,\n",
       " 6967,\n",
       " 6231,\n",
       " 1997,\n",
       " 4828,\n",
       " 2007,\n",
       " 2608,\n",
       " 2039,\n",
       " 14995,\n",
       " 6924,\n",
       " 2007,\n",
       " 1996,\n",
       " 5722,\n",
       " 1000,\n",
       " 2310,\n",
       " 3490,\n",
       " 2618,\n",
       " 4748,\n",
       " 2033,\n",
       " 18168,\n",
       " 5267,\n",
       " 1000,\n",
       " 1012,\n",
       " 2279,\n",
       " 2000,\n",
       " 1996,\n",
       " 2364,\n",
       " 2311,\n",
       " 2003,\n",
       " 1996,\n",
       " 13546,\n",
       " 1997,\n",
       " 1996,\n",
       " 6730,\n",
       " 2540,\n",
       " 1012,\n",
       " 3202,\n",
       " 2369,\n",
       " 1996,\n",
       " 13546,\n",
       " 2003,\n",
       " 1996,\n",
       " 24665,\n",
       " 23052,\n",
       " 1010,\n",
       " 1037,\n",
       " 14042,\n",
       " 2173,\n",
       " 1997,\n",
       " 7083,\n",
       " 1998,\n",
       " 9185,\n",
       " 1012,\n",
       " 2009,\n",
       " 2003,\n",
       " 1037,\n",
       " 15059,\n",
       " 1997,\n",
       " 1996,\n",
       " 24665,\n",
       " 23052,\n",
       " 2012,\n",
       " 10223,\n",
       " 26371,\n",
       " 1010,\n",
       " 2605,\n",
       " 2073,\n",
       " 1996,\n",
       " 6261,\n",
       " 2984,\n",
       " 22353,\n",
       " 2135,\n",
       " 2596,\n",
       " 2000,\n",
       " 3002,\n",
       " 16595,\n",
       " 9648,\n",
       " 4674,\n",
       " 2061,\n",
       " 12083,\n",
       " 9711,\n",
       " 2271,\n",
       " 1999,\n",
       " 8517,\n",
       " 1012,\n",
       " 2012,\n",
       " 1996,\n",
       " 2203,\n",
       " 1997,\n",
       " 1996,\n",
       " 2364,\n",
       " 3298,\n",
       " 1006,\n",
       " 1998,\n",
       " 1999,\n",
       " 1037,\n",
       " 3622,\n",
       " 2240,\n",
       " 2008,\n",
       " 8539,\n",
       " 2083,\n",
       " 1017,\n",
       " 11342,\n",
       " 1998,\n",
       " 1996,\n",
       " 2751,\n",
       " 8514,\n",
       " 1007,\n",
       " 1010,\n",
       " 2003,\n",
       " 1037,\n",
       " 3722,\n",
       " 1010,\n",
       " 2715,\n",
       " 2962,\n",
       " 6231,\n",
       " 1997,\n",
       " 2984,\n",
       " 1012,\n",
       " 102]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = datasets[\"train\"][0]\n",
    "tokenized_example=tokenizer(example[\"question\"], example[\"context\"])\n",
    "tokenized_example[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, None, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, None]\n"
     ]
    }
   ],
   "source": [
    "sequence_ids = tokenized_example.sequence_ids()\n",
    "print(sequence_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, example in enumerate(datasets[\"train\"]):\n",
    "    if len(tokenizer(example[\"question\"], example[\"context\"])[\"input_ids\"]) > 384:\n",
    "        break\n",
    "example = datasets[\"train\"][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "396"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer(example[\"question\"], example[\"context\"])[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_length = 384 # 输入feature的最大长度，question和context拼接之后\n",
    "doc_stride = 128 # 2个切片之间的重合token数量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer(example[\"question\"], example[\"context\"], max_length=max_length, truncation=\"only_second\")[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenized_example = tokenizer(\n",
    "    example[\"question\"],\n",
    "    example[\"context\"],\n",
    "    max_length=max_length,\n",
    "    truncation=\"only_second\",\n",
    "    return_overflowing_tokens=True,\n",
    "    stride=doc_stride\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[384, 157]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(x) for x in tokenized_example[\"input_ids\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "切片: 0\n",
      "[CLS] how many wins does the notre dame men's basketball team have? [SEP] the men's basketball team has over 1, 600 wins, one of only 12 schools who have reached that mark, and have appeared in 28 ncaa tournaments. former player austin carr holds the record for most points scored in a single game of the tournament with 61. although the team has never won the ncaa tournament, they were named by the helms athletic foundation as national champions twice. the team has orchestrated a number of upsets of number one ranked teams, the most notable of which was ending ucla's record 88 - game winning streak in 1974. the team has beaten an additional eight number - one teams, and those nine wins rank second, to ucla's 10, all - time in wins against the top team. the team plays in newly renovated purcell pavilion ( within the edmund p. joyce center ), which reopened for the beginning of the 2009 – 2010 season. the team is coached by mike brey, who, as of the 2014 – 15 season, his fifteenth at notre dame, has achieved a 332 - 165 record. in 2009 they were invited to the nit, where they advanced to the semifinals but were beaten by penn state who went on and beat baylor in the championship. the 2010 – 11 team concluded its regular season ranked number seven in the country, with a record of 25 – 5, brey's fifth straight 20 - win season, and a second - place finish in the big east. during the 2014 - 15 season, the team went 32 - 6 and won the acc conference tournament, later advancing to the elite 8, where the fighting irish lost on a missed buzzer - beater against then undefeated kentucky. led by nba draft picks jerian grant and pat connaughton, the fighting irish beat the eventual national champion duke blue devils twice during the season. the 32 wins were [SEP]\n",
      "切片: 1\n",
      "[CLS] how many wins does the notre dame men's basketball team have? [SEP] championship. the 2010 – 11 team concluded its regular season ranked number seven in the country, with a record of 25 – 5, brey's fifth straight 20 - win season, and a second - place finish in the big east. during the 2014 - 15 season, the team went 32 - 6 and won the acc conference tournament, later advancing to the elite 8, where the fighting irish lost on a missed buzzer - beater against then undefeated kentucky. led by nba draft picks jerian grant and pat connaughton, the fighting irish beat the eventual national champion duke blue devils twice during the season. the 32 wins were the most by the fighting irish team since 1908 - 09. [SEP]\n"
     ]
    }
   ],
   "source": [
    "for i, x in enumerate(tokenized_example[\"input_ids\"][:2]):\n",
    "    print(\"切片: {}\".format(i))\n",
    "    print(tokenizer.decode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenized_example = tokenizer(\n",
    "    example[\"question\"],\n",
    "    example[\"context\"],\n",
    "    max_length=max_length,\n",
    "    truncation=\"only_second\",\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    "    stride=doc_stride\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0), (0, 3), (4, 8), (9, 13), (14, 18), (19, 22), (23, 28), (29, 33), (34, 37), (37, 38), (38, 39), (40, 50), (51, 55), (56, 60), (60, 61), (0, 0), (1093, 1105), (1105, 1106), (1107, 1110), (1111, 1115), (1115, 1116), (1116, 1118), (1119, 1123), (1124, 1133), (1134, 1137), (1138, 1145), (1146, 1152), (1153, 1159), (1160, 1166), (1167, 1172)]\n"
     ]
    }
   ],
   "source": [
    "# 打印切片前后位置下标的对应关系\n",
    "print(tokenized_example[\"offset_mapping\"][1][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how How\n"
     ]
    }
   ],
   "source": [
    "first_token_id = tokenized_example[\"input_ids\"][0][1] #2129\n",
    "offsets = tokenized_example[\"offset_mapping\"][0][1] #(0, 3)\n",
    "print(tokenizer.convert_ids_to_tokens([first_token_id])[0], example[\"question\"][offsets[0]:offsets[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "answers = example[\"answers\"] #答案{'answer_start': [30], 'text': ['over 1,600']}\n",
    "start_char = answers[\"answer_start\"][0] #答案起始位置30\n",
    "end_char = start_char + len(answers[\"text\"][0])#答案终止位置40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 找到当前文本的Start token index.\n",
    "token_start_index = 0 #得到context在句子1和句子2拼接后的句子中的起始位置\n",
    "while sequence_ids[token_start_index] != 1: #sequence_ids区分question和context\n",
    "    token_start_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 找到当前文本的End token index.\n",
    "token_end_index = len(tokenized_example[\"input_ids\"][0]) - 1#得到context在句子1和句子2拼接后的句子中的终止位置，可能还要去掉一些padding\n",
    "while token_end_index>=len(sequence_ids) or sequence_ids[token_end_index] != 1:\n",
    "    token_end_index -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_position: 23, end_position: 26\n"
     ]
    }
   ],
   "source": [
    "# 检测答案是否在文本区间的外部，这种情况下意味着该样本的数据标注在CLS token位置。\n",
    "offsets = tokenized_example[\"offset_mapping\"][0]\n",
    "if (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char): #答案在文本内\n",
    "    # 将token_start_index和token_end_index移动到answer所在位置的两侧.\n",
    "    # 注意：答案在最末尾的边界条件.\n",
    "    while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "        token_start_index += 1 #之前的token_start_index在context的第一个token位置\n",
    "    start_position = token_start_index - 1\n",
    "    while offsets[token_end_index][1] >= end_char:\n",
    "        token_end_index -= 1#之前的token_end_index在context的最后一个token位置\n",
    "    end_position = token_end_index + 1\n",
    "    print(\"start_position: {}, end_position: {}\".format(start_position, end_position))\n",
    "else: #答案在文本外\n",
    "    print(\"The answer is not in this feature.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over 1, 600\n",
      "over 1,600\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokenized_example[\"input_ids\"][0][start_position: end_position+1]))\n",
    "print(answers[\"text\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pad_on_right = tokenizer.padding_side == \"right\" #context在右边"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prepare_train_features(examples):\n",
    "    # 既要对examples进行truncation（截断）和padding（补全）还要还要保留所有信息，所以要用的切片的方法。\n",
    "    # 每一个超长文本example会被切片成多个输入，相邻两个输入之间会有交集。\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # 我们使用overflow_to_sample_mapping参数来映射切片片ID到原始ID。\n",
    "    # 比如有2个expamples被切成4片，那么对应是[0, 0, 1, 1]，前两片对应原来的第一个example。\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    # offset_mapping也对应4片\n",
    "    # offset_mapping参数帮助我们映射到原始输入，由于答案标注在原始输入上，所以有助于我们找到答案的起始和结束位置。\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "    # 重新标注数据\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        # 对每一片进行处理\n",
    "        # 将无答案的样本标注到CLS上\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "        # 区分question和context\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "\n",
    "        # 拿到原始的example 下标.\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = examples[\"answers\"][sample_index]\n",
    "        # 如果没有答案，则使用CLS所在的位置为答案.\n",
    "        if len(answers[\"answer_start\"]) == 0:\n",
    "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "        else:\n",
    "            # 答案的character级别Start/end位置.\n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "            # 找到token级别的index start.\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
    "                token_start_index += 1\n",
    "\n",
    "            # 找到token级别的index end.\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
    "                token_end_index -= 1\n",
    "\n",
    "            # 检测答案是否超出文本长度，超出的话也使用CLS index作为标注.\n",
    "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                # 如果不超出则找到答案token的start和end位置。.\n",
    "                # Note: we could go after the last offset if the answer is the last word (edge case).\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:/Users/96212/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453\\cache-c012c1f472b79710.arrow\n",
      "Loading cached processed dataset at C:/Users/96212/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453\\cache-3d69908f41919809.arrow\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = datasets.map(prepare_train_features, batched=True, remove_columns=datasets[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nAutoModelForQuestionAnswering requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [75], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[39mfrom\u001B[39;00m \u001B[39mtransformers\u001B[39;00m \u001B[39mimport\u001B[39;00m AutoModelForQuestionAnswering\n\u001B[1;32m----> 2\u001B[0m model \u001B[39m=\u001B[39m AutoModelForQuestionAnswering\u001B[39m.\u001B[39;49mfrom_pretrained(model_checkpoint)\n",
      "File \u001B[1;32md:\\Users\\96212\\anaconda3\\envs\\paddle\\lib\\site-packages\\transformers\\utils\\import_utils.py:992\u001B[0m, in \u001B[0;36mDummyObject.__getattr__\u001B[1;34m(cls, key)\u001B[0m\n\u001B[0;32m    990\u001B[0m \u001B[39mif\u001B[39;00m key\u001B[39m.\u001B[39mstartswith(\u001B[39m\"\u001B[39m\u001B[39m_\u001B[39m\u001B[39m\"\u001B[39m):\n\u001B[0;32m    991\u001B[0m     \u001B[39mreturn\u001B[39;00m \u001B[39msuper\u001B[39m()\u001B[39m.\u001B[39m\u001B[39m__getattr__\u001B[39m(\u001B[39mcls\u001B[39m, key)\n\u001B[1;32m--> 992\u001B[0m requires_backends(\u001B[39mcls\u001B[39;49m, \u001B[39mcls\u001B[39;49m\u001B[39m.\u001B[39;49m_backends)\n",
      "File \u001B[1;32md:\\Users\\96212\\anaconda3\\envs\\paddle\\lib\\site-packages\\transformers\\utils\\import_utils.py:980\u001B[0m, in \u001B[0;36mrequires_backends\u001B[1;34m(obj, backends)\u001B[0m\n\u001B[0;32m    978\u001B[0m failed \u001B[39m=\u001B[39m [msg\u001B[39m.\u001B[39mformat(name) \u001B[39mfor\u001B[39;00m available, msg \u001B[39min\u001B[39;00m checks \u001B[39mif\u001B[39;00m \u001B[39mnot\u001B[39;00m available()]\n\u001B[0;32m    979\u001B[0m \u001B[39mif\u001B[39;00m failed:\n\u001B[1;32m--> 980\u001B[0m     \u001B[39mraise\u001B[39;00m \u001B[39mImportError\u001B[39;00m(\u001B[39m\"\u001B[39m\u001B[39m\"\u001B[39m\u001B[39m.\u001B[39mjoin(failed))\n",
      "\u001B[1;31mImportError\u001B[0m: \nAutoModelForQuestionAnswering requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "token_end_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(sequence_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "token_end_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('paddle')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "ada01964033a61a0dd4d26d0d4b5c038ced59bbb116c97c5ca6d6bc70c53719a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
