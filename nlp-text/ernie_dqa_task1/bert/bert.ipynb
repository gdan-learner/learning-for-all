{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in d:\\users\\96212\\anaconda3\\envs\\paddle\\lib\\site-packages (2.7.1)\n",
      "Requirement already satisfied: transformers in d:\\users\\96212\\anaconda3\\envs\\paddle\\lib\\site-packages (4.24.0)\n",
      "Requirement already satisfied: multiprocess in d:\\users\\96212\\anaconda3\\envs\\paddle\\lib\\site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: aiohttp in d:\\users\\96212\\anaconda3\\envs\\paddle\\lib\\site-packages (from datasets) (3.8.3)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in d:\\users\\96212\\anaconda3\\envs\\paddle\\lib\\site-packages (from datasets) (10.0.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in d:\\users\\96212\\anaconda3\\envs\\paddle\\lib\\site-packages (from datasets) (4.64.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in d:\\users\\96212\\anaconda3\\envs\\paddle\\lib\\site-packages (from datasets) (2.28.1)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in d:\\users\\96212\\anaconda3\\envs\\paddle\\lib\\site-packages (from datasets) (2022.11.0)\n",
      "Requirement already satisfied: packaging in d:\\users\\96212\\anaconda3\\envs\\paddle\\lib\\site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: xxhash in d:\\users\\96212\\anaconda3\\envs\\paddle\\lib\\site-packages (from datasets) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\users\\96212\\anaconda3\\envs\\paddle\\lib\\site-packages (from datasets) (1.23.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in d:\\users\\96212\\anaconda3\\envs\\paddle\\lib\\site-packages (from datasets) (0.11.0)\n",
      "Requirement already satisfied: pandas in d:\\users\\96212\\anaconda3\\envs\\paddle\\lib\\site-packages (from datasets) (1.5.2)\n",
      "Requirement already satisfied: responses<0.19 in d:\\users\\96212\\anaconda3\\envs\\paddle\\lib\\site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\users\\96212\\anaconda3\\envs\\paddle\\lib\\site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: dill<0.3.7 in d:\\users\\96212\\anaconda3\\envs\\paddle\\lib\\site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: filelock in d:\\users\\96212\\anaconda3\\envs\\paddle\\lib\\site-packages (from transformers) (3.8.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in d:\\users\\96212\\anaconda3\\envs\\paddle\\lib\\site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\users\\96212\\anaconda3\\envs\\paddle\\lib\\site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\users\\96212\\anaconda3\\envs\\paddle\\lib\\site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in d:\\users\\96212\\anaconda3\\envs\\paddle\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in d:\\users\\96212\\anaconda3\\envs\\paddle\\lib\\site-packages (from aiohttp->datasets) (1.8.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\users\\96212\\anaconda3\\envs\\paddle\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\users\\96212\\anaconda3\\envs\\paddle\\lib\\site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\users\\96212\\anaconda3\\envs\\paddle\\lib\\site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in d:\\users\\96212\\anaconda3\\envs\\paddle\\lib\\site-packages (from aiohttp->datasets) (2.1.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\users\\96212\\anaconda3\\envs\\paddle\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in d:\\users\\96212\\anaconda3\\envs\\paddle\\lib\\site-packages (from packaging->datasets) (3.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\users\\96212\\anaconda3\\envs\\paddle\\lib\\site-packages (from requests>=2.19.0->datasets) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\users\\96212\\anaconda3\\envs\\paddle\\lib\\site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\users\\96212\\anaconda3\\envs\\paddle\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.12)\n",
      "Requirement already satisfied: colorama in d:\\users\\96212\\anaconda3\\envs\\paddle\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\users\\96212\\anaconda3\\envs\\paddle\\lib\\site-packages (from pandas->datasets) (2022.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in d:\\users\\96212\\anaconda3\\envs\\paddle\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in d:\\users\\96212\\anaconda3\\envs\\paddle\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets transformers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1.Preparation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset squad (C:/Users/96212/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c438864387a547158f822f8fc907d59a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# squad_v2等于True或者False分别代表使用SQUAD v1 或者 SQUAD v2。\n",
    "# 如果您使用的是其他数据集，那么True代表的是：模型可以回答“不可回答”问题，也就是部分问题不给出答案，而False则代表所有问题必须回答。\n",
    "squad_v2 = False\n",
    "# 下载数据（确保有网络）\n",
    "datasets = load_dataset(\"squad_v2\" if squad_v2 else \"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "data": {
      "text/plain": "{'id': '5733be284776f41900661182',\n 'title': 'University_of_Notre_Dame',\n 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',\n 'answers': {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}}"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"][0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "from datasets import ClassLabel, Sequence\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "\n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "        elif isinstance(typ, Sequence) and isinstance(typ.feature, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda x: [typ.feature.names[i] for i in x])\n",
    "    display(HTML(df.to_html()))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>title</th>\n      <th>context</th>\n      <th>question</th>\n      <th>answers</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>56d00567234ae51400d9c277</td>\n      <td>New_York_City</td>\n      <td>New York has been described as the \"Capital of Baseball\". There have been 35 Major League Baseball World Series and 73 pennants won by New York teams. It is one of only five metro areas (Los Angeles, Chicago, Baltimore–Washington, and the San Francisco Bay Area being the others) to have two baseball teams. Additionally, there have been 14 World Series in which two New York City teams played each other, known as a Subway Series and occurring most recently in 2000. No other metropolitan area has had this happen more than once (Chicago in 1906, St. Louis in 1944, and the San Francisco Bay Area in 1989). The city's two current Major League Baseball teams are the New York Mets, who play at Citi Field in Queens, and the New York Yankees, who play at Yankee Stadium in the Bronx. who compete in six games of interleague play every regular season that has also come to be called the Subway Series. The Yankees have won a record 27 championships, while the Mets have won the World Series twice. The city also was once home to the Brooklyn Dodgers (now the Los Angeles Dodgers), who won the World Series once, and the New York Giants (now the San Francisco Giants), who won the World Series five times. Both teams moved to California in 1958. There are also two Minor League Baseball teams in the city, the Brooklyn Cyclones and Staten Island Yankees.</td>\n      <td>How many minor league baseball teams are there in NYC?</td>\n      <td>{'text': ['two'], 'answer_start': [288]}</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(datasets[\"train\"], num_examples=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. data process"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "data": {
      "text/plain": "[101,\n 2000,\n 3183,\n 2106,\n 1996,\n 6261,\n 2984,\n 9382,\n 3711,\n 1999,\n 8517,\n 1999,\n 10223,\n 26371,\n 2605,\n 1029,\n 102,\n 6549,\n 2135,\n 1010,\n 1996,\n 2082,\n 2038,\n 1037,\n 3234,\n 2839,\n 1012,\n 10234,\n 1996,\n 2364,\n 2311,\n 1005,\n 1055,\n 2751,\n 8514,\n 2003,\n 1037,\n 3585,\n 6231,\n 1997,\n 1996,\n 6261,\n 2984,\n 1012,\n 3202,\n 1999,\n 2392,\n 1997,\n 1996,\n 2364,\n 2311,\n 1998,\n 5307,\n 2009,\n 1010,\n 2003,\n 1037,\n 6967,\n 6231,\n 1997,\n 4828,\n 2007,\n 2608,\n 2039,\n 14995,\n 6924,\n 2007,\n 1996,\n 5722,\n 1000,\n 2310,\n 3490,\n 2618,\n 4748,\n 2033,\n 18168,\n 5267,\n 1000,\n 1012,\n 2279,\n 2000,\n 1996,\n 2364,\n 2311,\n 2003,\n 1996,\n 13546,\n 1997,\n 1996,\n 6730,\n 2540,\n 1012,\n 3202,\n 2369,\n 1996,\n 13546,\n 2003,\n 1996,\n 24665,\n 23052,\n 1010,\n 1037,\n 14042,\n 2173,\n 1997,\n 7083,\n 1998,\n 9185,\n 1012,\n 2009,\n 2003,\n 1037,\n 15059,\n 1997,\n 1996,\n 24665,\n 23052,\n 2012,\n 10223,\n 26371,\n 1010,\n 2605,\n 2073,\n 1996,\n 6261,\n 2984,\n 22353,\n 2135,\n 2596,\n 2000,\n 3002,\n 16595,\n 9648,\n 4674,\n 2061,\n 12083,\n 9711,\n 2271,\n 1999,\n 8517,\n 1012,\n 2012,\n 1996,\n 2203,\n 1997,\n 1996,\n 2364,\n 3298,\n 1006,\n 1998,\n 1999,\n 1037,\n 3622,\n 2240,\n 2008,\n 8539,\n 2083,\n 1017,\n 11342,\n 1998,\n 1996,\n 2751,\n 8514,\n 1007,\n 1010,\n 2003,\n 1037,\n 3722,\n 1010,\n 2715,\n 2962,\n 6231,\n 1997,\n 2984,\n 1012,\n 102]"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = datasets[\"train\"][0]\n",
    "tokenized_example=tokenizer(example[\"question\"], example[\"context\"])\n",
    "tokenized_example[\"input_ids\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, None, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, None]\n"
     ]
    }
   ],
   "source": [
    "sequence_ids = tokenized_example.sequence_ids()\n",
    "print(sequence_ids)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "for i, example in enumerate(datasets[\"train\"]):\n",
    "    if len(tokenizer(example[\"question\"], example[\"context\"])[\"input_ids\"]) > 384:\n",
    "        break\n",
    "example = datasets[\"train\"][i]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "data": {
      "text/plain": "396"
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer(example[\"question\"], example[\"context\"])[\"input_ids\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "max_length = 384 # 输入feature的最大长度，question和context拼接之后\n",
    "doc_stride = 128 # 2个切片之间的重合token数量。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "data": {
      "text/plain": "384"
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer(example[\"question\"], example[\"context\"], max_length=max_length, truncation=\"only_second\")[\"input_ids\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "tokenized_example = tokenizer(\n",
    "    example[\"question\"],\n",
    "    example[\"context\"],\n",
    "    max_length=max_length,\n",
    "    truncation=\"only_second\",\n",
    "    return_overflowing_tokens=True,\n",
    "    stride=doc_stride\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "data": {
      "text/plain": "[384, 157]"
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(x) for x in tokenized_example[\"input_ids\"]]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "切片: 0\n",
      "[CLS] how many wins does the notre dame men's basketball team have? [SEP] the men's basketball team has over 1, 600 wins, one of only 12 schools who have reached that mark, and have appeared in 28 ncaa tournaments. former player austin carr holds the record for most points scored in a single game of the tournament with 61. although the team has never won the ncaa tournament, they were named by the helms athletic foundation as national champions twice. the team has orchestrated a number of upsets of number one ranked teams, the most notable of which was ending ucla's record 88 - game winning streak in 1974. the team has beaten an additional eight number - one teams, and those nine wins rank second, to ucla's 10, all - time in wins against the top team. the team plays in newly renovated purcell pavilion ( within the edmund p. joyce center ), which reopened for the beginning of the 2009 – 2010 season. the team is coached by mike brey, who, as of the 2014 – 15 season, his fifteenth at notre dame, has achieved a 332 - 165 record. in 2009 they were invited to the nit, where they advanced to the semifinals but were beaten by penn state who went on and beat baylor in the championship. the 2010 – 11 team concluded its regular season ranked number seven in the country, with a record of 25 – 5, brey's fifth straight 20 - win season, and a second - place finish in the big east. during the 2014 - 15 season, the team went 32 - 6 and won the acc conference tournament, later advancing to the elite 8, where the fighting irish lost on a missed buzzer - beater against then undefeated kentucky. led by nba draft picks jerian grant and pat connaughton, the fighting irish beat the eventual national champion duke blue devils twice during the season. the 32 wins were [SEP]\n",
      "切片: 1\n",
      "[CLS] how many wins does the notre dame men's basketball team have? [SEP] championship. the 2010 – 11 team concluded its regular season ranked number seven in the country, with a record of 25 – 5, brey's fifth straight 20 - win season, and a second - place finish in the big east. during the 2014 - 15 season, the team went 32 - 6 and won the acc conference tournament, later advancing to the elite 8, where the fighting irish lost on a missed buzzer - beater against then undefeated kentucky. led by nba draft picks jerian grant and pat connaughton, the fighting irish beat the eventual national champion duke blue devils twice during the season. the 32 wins were the most by the fighting irish team since 1908 - 09. [SEP]\n"
     ]
    }
   ],
   "source": [
    "for i, x in enumerate(tokenized_example[\"input_ids\"][:2]):\n",
    "    print(\"切片: {}\".format(i))\n",
    "    print(tokenizer.decode(x))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "tokenized_example = tokenizer(\n",
    "    example[\"question\"],\n",
    "    example[\"context\"],\n",
    "    max_length=max_length,\n",
    "    truncation=\"only_second\",\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    "    stride=doc_stride\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0), (0, 3), (4, 8), (9, 13), (14, 18), (19, 22), (23, 28), (29, 33), (34, 37), (37, 38), (38, 39), (40, 50), (51, 55), (56, 60), (60, 61), (0, 0), (1093, 1105), (1105, 1106), (1107, 1110), (1111, 1115), (1115, 1116), (1116, 1118), (1119, 1123), (1124, 1133), (1134, 1137), (1138, 1145), (1146, 1152), (1153, 1159), (1160, 1166), (1167, 1172)]\n"
     ]
    }
   ],
   "source": [
    "# 打印切片前后位置下标的对应关系\n",
    "print(tokenized_example[\"offset_mapping\"][1][:30])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how How\n"
     ]
    }
   ],
   "source": [
    "first_token_id = tokenized_example[\"input_ids\"][0][1] #2129\n",
    "offsets = tokenized_example[\"offset_mapping\"][0][1] #(0, 3)\n",
    "print(tokenizer.convert_ids_to_tokens([first_token_id])[0], example[\"question\"][offsets[0]:offsets[1]])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "answers = example[\"answers\"] #答案{'answer_start': [30], 'text': ['over 1,600']}\n",
    "start_char = answers[\"answer_start\"][0] #答案起始位置30\n",
    "end_char = start_char + len(answers[\"text\"][0])#答案终止位置40"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "# 找到当前文本的Start token index.\n",
    "token_start_index = 0 #得到context在句子1和句子2拼接后的句子中的起始位置\n",
    "while sequence_ids[token_start_index] != 1: #sequence_ids区分question和context\n",
    "    token_start_index += 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "# 找到当前文本的End token index.\n",
    "token_end_index = len(tokenized_example[\"input_ids\"][0]) - 1#得到context在句子1和句子2拼接后的句子中的终止位置，可能还要去掉一些padding\n",
    "while token_end_index>=len(sequence_ids) or sequence_ids[token_end_index] != 1:\n",
    "    token_end_index -= 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_position: 23, end_position: 26\n"
     ]
    }
   ],
   "source": [
    "# 检测答案是否在文本区间的外部，这种情况下意味着该样本的数据标注在CLS token位置。\n",
    "offsets = tokenized_example[\"offset_mapping\"][0]\n",
    "if (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char): #答案在文本内\n",
    "    # 将token_start_index和token_end_index移动到answer所在位置的两侧.\n",
    "    # 注意：答案在最末尾的边界条件.\n",
    "    while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "        token_start_index += 1 #之前的token_start_index在context的第一个token位置\n",
    "    start_position = token_start_index - 1\n",
    "    while offsets[token_end_index][1] >= end_char:\n",
    "        token_end_index -= 1#之前的token_end_index在context的最后一个token位置\n",
    "    end_position = token_end_index + 1\n",
    "    print(\"start_position: {}, end_position: {}\".format(start_position, end_position))\n",
    "else: #答案在文本外\n",
    "    print(\"The answer is not in this feature.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over 1, 600\n",
      "over 1,600\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokenized_example[\"input_ids\"][0][start_position: end_position+1]))\n",
    "print(answers[\"text\"][0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "pad_on_right = tokenizer.padding_side == \"right\" #context在右边"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "def prepare_train_features(examples):\n",
    "    # 既要对examples进行truncation（截断）和padding（补全）还要还要保留所有信息，所以要用的切片的方法。\n",
    "    # 每一个超长文本example会被切片成多个输入，相邻两个输入之间会有交集。\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # 我们使用overflow_to_sample_mapping参数来映射切片片ID到原始ID。\n",
    "    # 比如有2个expamples被切成4片，那么对应是[0, 0, 1, 1]，前两片对应原来的第一个example。\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    # offset_mapping也对应4片\n",
    "    # offset_mapping参数帮助我们映射到原始输入，由于答案标注在原始输入上，所以有助于我们找到答案的起始和结束位置。\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "    # 重新标注数据\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        # 对每一片进行处理\n",
    "        # 将无答案的样本标注到CLS上\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "        # 区分question和context\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "\n",
    "        # 拿到原始的example 下标.\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = examples[\"answers\"][sample_index]\n",
    "        # 如果没有答案，则使用CLS所在的位置为答案.\n",
    "        if len(answers[\"answer_start\"]) == 0:\n",
    "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "        else:\n",
    "            # 答案的character级别Start/end位置.\n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "            # 找到token级别的index start.\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
    "                token_start_index += 1\n",
    "\n",
    "            # 找到token级别的index end.\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
    "                token_end_index -= 1\n",
    "\n",
    "            # 检测答案是否超出文本长度，超出的话也使用CLS index作为标注.\n",
    "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                # 如果不超出则找到答案token的start和end位置。.\n",
    "                # Note: we could go after the last offset if the answer is the last word (edge case).\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "\n",
    "    return tokenized_examples"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:/Users/96212/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453\\cache-c012c1f472b79710.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/11 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9e1faa2aeff341c082ceeb2cee0edbe8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = datasets.map(prepare_train_features, batched=True, remove_columns=datasets[\"train\"].column_names)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nAutoModelForQuestionAnswering requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [89], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AutoModelForQuestionAnswering\n\u001B[1;32m----> 2\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mAutoModelForQuestionAnswering\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m(model_checkpoint)\n",
      "File \u001B[1;32mD:\\Users\\96212\\anaconda3\\envs\\paddle\\lib\\site-packages\\transformers\\utils\\import_utils.py:992\u001B[0m, in \u001B[0;36mDummyObject.__getattr__\u001B[1;34m(cls, key)\u001B[0m\n\u001B[0;32m    990\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m key\u001B[38;5;241m.\u001B[39mstartswith(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m    991\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__getattr__\u001B[39m(\u001B[38;5;28mcls\u001B[39m, key)\n\u001B[1;32m--> 992\u001B[0m \u001B[43mrequires_backends\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_backends\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Users\\96212\\anaconda3\\envs\\paddle\\lib\\site-packages\\transformers\\utils\\import_utils.py:980\u001B[0m, in \u001B[0;36mrequires_backends\u001B[1;34m(obj, backends)\u001B[0m\n\u001B[0;32m    978\u001B[0m failed \u001B[38;5;241m=\u001B[39m [msg\u001B[38;5;241m.\u001B[39mformat(name) \u001B[38;5;28;01mfor\u001B[39;00m available, msg \u001B[38;5;129;01min\u001B[39;00m checks \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m available()]\n\u001B[0;32m    979\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m failed:\n\u001B[1;32m--> 980\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(failed))\n",
      "\u001B[1;31mImportError\u001B[0m: \nAutoModelForQuestionAnswering requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "token_end_index"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(sequence_ids)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "token_end_index"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
