{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "from PIL import Image\n",
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 3., 6.]),\n",
       " tensor([0., 3.]),\n",
       " (tensor([[0., 0., 0.],\n",
       "          [3., 3., 3.]]),\n",
       "  tensor([[0., 3., 6.],\n",
       "          [0., 3., 6.]])))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shifts_x = torch.arange(0, 3, dtype=torch.float32) * 3\n",
    "shifts_y = torch.arange(0, 2, dtype=torch.float32) * 3\n",
    "shifts_x,shifts_y,torch.meshgrid(shifts_y, shifts_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [1]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dw = torch.tensor([[1],[2]])\n",
    "dw = torch.clamp(dw, max=1)\n",
    "dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'int' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32me:\\save-github\\deep-learning-all\\object-detection\\01_faster_rcnn\\jupyter\\my_dataset.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/save-github/deep-learning-all/object-detection/01_faster_rcnn/jupyter/my_dataset.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m torch\u001b[39m.\u001b[39mempty(\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39muniform_(\u001b[39m0.\u001b[39m, \u001b[39mfloat\u001b[39m(\u001b[39mlen\u001b[39;49m(\u001b[39m800\u001b[39;49m)))\u001b[39m.\u001b[39mitem()\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'int' has no len()"
     ]
    }
   ],
   "source": [
    "torch.empty(1).uniform_(0., float(1)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(torch.empty(1).uniform_(0., float(1)).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(([1, 2], [3, 4]),\n",
       " ({'boxes': [[106.0, 25.0, 498.0, 372.0]]},\n",
       "  {'boxes': [[111.0, 33.0, 45.0, 66.0]]}))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = [([1,2],{'boxes': [[106.,  25., 498., 372.]]}),([3,4],{'boxes': [[111.,  33., 45., 66.]]})]\n",
    "tuple(zip(*(batch)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VOCDataSet(Dataset):\n",
    "    \"\"\"读取解析PASCAL VOC2007/2012数据集\"\"\"\n",
    "\n",
    "    def __init__(self, voc_root, year=\"2012\", transforms=None, txt_name: str = \"train.txt\"):\n",
    "        assert year in [\"2007\", \"2012\"], \"year must be in ['2007', '2012']\"\n",
    "        # 增加容错能力\n",
    "        if \"VOCdevkit\" in voc_root:\n",
    "            self.root = os.path.join(voc_root, f\"VOC{year}\")\n",
    "        else:\n",
    "            self.root = os.path.join(voc_root, \"VOCdevkit\", f\"VOC{year}\")\n",
    "        self.img_root = os.path.join(self.root, \"JPEGImages\")\n",
    "        self.annotations_root = os.path.join(self.root, \"Annotations\")\n",
    "\n",
    "        # read train.txt or val.txt file\n",
    "        txt_path = os.path.join(self.root, \"ImageSets\", \"Main\", txt_name)\n",
    "        assert os.path.exists(txt_path), \"not found {} file.\".format(txt_name)\n",
    "\n",
    "        with open(txt_path) as read:\n",
    "            xml_list = [os.path.join(self.annotations_root, line.strip() + \".xml\")\n",
    "                        for line in read.readlines() if len(line.strip()) > 0]\n",
    "\n",
    "        self.xml_list = []\n",
    "        # check file\n",
    "        for xml_path in xml_list:\n",
    "            if os.path.exists(xml_path) is False:\n",
    "                print(f\"Warning: not found '{xml_path}', skip this annotation file.\")\n",
    "                continue\n",
    "\n",
    "            # check for targets\n",
    "            with open(xml_path) as fid:\n",
    "                xml_str = fid.read()\n",
    "            xml = etree.fromstring(xml_str)\n",
    "            data = self.parse_xml_to_dict(xml)[\"annotation\"]\n",
    "            if \"object\" not in data:\n",
    "                print(f\"INFO: no objects in {xml_path}, skip this annotation file.\")\n",
    "                continue\n",
    "\n",
    "            self.xml_list.append(xml_path)\n",
    "\n",
    "        assert len(self.xml_list) > 0, \"in '{}' file does not find any information.\".format(txt_path)\n",
    "\n",
    "        # read class_indict\n",
    "        json_file = 'data/pascal_voc_classes.json'\n",
    "        assert os.path.exists(json_file), \"{} file not exist.\".format(json_file)\n",
    "        with open(json_file, 'r') as f:\n",
    "            self.class_dict = json.load(f)\n",
    "\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.xml_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # read xml\n",
    "        xml_path = self.xml_list[idx]\n",
    "        with open(xml_path) as fid:\n",
    "            xml_str = fid.read()\n",
    "        xml = etree.fromstring(xml_str)\n",
    "        data = self.parse_xml_to_dict(xml)[\"annotation\"]\n",
    "        img_path = os.path.join(self.img_root, data[\"filename\"])\n",
    "        image = Image.open(img_path)\n",
    "        if image.format != \"JPEG\":\n",
    "            raise ValueError(\"Image '{}' format not JPEG\".format(img_path))\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        iscrowd = []\n",
    "        assert \"object\" in data, \"{} lack of object information.\".format(xml_path)\n",
    "        for obj in data[\"object\"]:\n",
    "            xmin = float(obj[\"bndbox\"][\"xmin\"])\n",
    "            xmax = float(obj[\"bndbox\"][\"xmax\"])\n",
    "            ymin = float(obj[\"bndbox\"][\"ymin\"])\n",
    "            ymax = float(obj[\"bndbox\"][\"ymax\"])\n",
    "\n",
    "            # 进一步检查数据，有的标注信息中可能有w或h为0的情况，这样的数据会导致计算回归loss为nan\n",
    "            if xmax <= xmin or ymax <= ymin:\n",
    "                print(\"Warning: in '{}' xml, there are some bbox w/h <=0\".format(xml_path))\n",
    "                continue\n",
    "\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "            labels.append(self.class_dict[obj[\"name\"]])\n",
    "            if \"difficult\" in obj:\n",
    "                iscrowd.append(int(obj[\"difficult\"]))\n",
    "            else:\n",
    "                iscrowd.append(0)\n",
    "\n",
    "        # convert everything into a torch.Tensor\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        iscrowd = torch.as_tensor(iscrowd, dtype=torch.int64)\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            image, target = self.transforms(image, target)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "    def get_height_and_width(self, idx):\n",
    "        # read xml\n",
    "        xml_path = self.xml_list[idx]\n",
    "        with open(xml_path) as fid:\n",
    "            xml_str = fid.read()\n",
    "        xml = etree.fromstring(xml_str)\n",
    "        data = self.parse_xml_to_dict(xml)[\"annotation\"]\n",
    "        data_height = int(data[\"size\"][\"height\"])\n",
    "        data_width = int(data[\"size\"][\"width\"])\n",
    "        return data_height, data_width\n",
    "\n",
    "    def parse_xml_to_dict(self, xml):\n",
    "        \"\"\"\n",
    "        将xml文件解析成字典形式，参考tensorflow的recursive_parse_xml_to_dict\n",
    "        Args:\n",
    "            xml: xml tree obtained by parsing XML file contents using lxml.etree\n",
    "\n",
    "        Returns:\n",
    "            Python dictionary holding XML contents.\n",
    "        \"\"\"\n",
    "\n",
    "        if len(xml) == 0:  # 遍历到底层，直接返回tag对应的信息\n",
    "            return {xml.tag: xml.text}\n",
    "\n",
    "        result = {}\n",
    "        for child in xml:\n",
    "            child_result = self.parse_xml_to_dict(child)  # 递归遍历标签信息\n",
    "            if child.tag != 'object':\n",
    "                result[child.tag] = child_result[child.tag]\n",
    "            else:\n",
    "                if child.tag not in result:  # 因为object可能有多个，所以需要放入列表里\n",
    "                    result[child.tag] = []\n",
    "                result[child.tag].append(child_result[child.tag])\n",
    "        return {xml.tag: result}\n",
    "\n",
    "    def coco_index(self, idx):\n",
    "        \"\"\"\n",
    "        该方法是专门为pycocotools统计标签信息准备，不对图像和标签作任何处理\n",
    "        由于不用去读取图片，可大幅缩减统计时间\n",
    "\n",
    "        Args:\n",
    "            idx: 输入需要获取图像的索引\n",
    "        \"\"\"\n",
    "        # read xml\n",
    "        xml_path = self.xml_list[idx]\n",
    "        with open(xml_path) as fid:\n",
    "            xml_str = fid.read()\n",
    "        xml = etree.fromstring(xml_str)\n",
    "        data = self.parse_xml_to_dict(xml)[\"annotation\"]\n",
    "        data_height = int(data[\"size\"][\"height\"])\n",
    "        data_width = int(data[\"size\"][\"width\"])\n",
    "        # img_path = os.path.join(self.img_root, data[\"filename\"])\n",
    "        # image = Image.open(img_path)\n",
    "        # if image.format != \"JPEG\":\n",
    "        #     raise ValueError(\"Image format not JPEG\")\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        iscrowd = []\n",
    "        for obj in data[\"object\"]:\n",
    "            xmin = float(obj[\"bndbox\"][\"xmin\"])\n",
    "            xmax = float(obj[\"bndbox\"][\"xmax\"])\n",
    "            ymin = float(obj[\"bndbox\"][\"ymin\"])\n",
    "            ymax = float(obj[\"bndbox\"][\"ymax\"])\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "            labels.append(self.class_dict[obj[\"name\"]])\n",
    "            iscrowd.append(int(obj[\"difficult\"]))\n",
    "\n",
    "        # convert everything into a torch.Tensor\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        iscrowd = torch.as_tensor(iscrowd, dtype=torch.int64)\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        return (data_height, data_width), target\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms, datasets, utils\n",
    "# import transforms\n",
    "# from draw_box_utils import draw_objs\n",
    "from PIL import Image\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as ts\n",
    "import random\n",
    "\n",
    "# read class_indict\n",
    "category_index = {}\n",
    "try:\n",
    "    json_file = open('data/pascal_voc_classes.json', 'r')\n",
    "    class_dict = json.load(json_file)\n",
    "    category_index = {str(v): str(k) for k, v in class_dict.items()}\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    exit(-1)\n",
    "\n",
    "data_transform = {\n",
    "    \"train\": transforms.Compose([transforms.ToTensor(),\n",
    "                                 transforms.RandomHorizontalFlip(0.5)]),\n",
    "    \"val\": transforms.Compose([transforms.ToTensor()])\n",
    "}\n",
    "\n",
    "voc_path = os.path.join(os.getcwd(), 'data')\n",
    "\n",
    "# load train data set\n",
    "train_data_set = VOCDataSet(voc_path, \"2007\", data_transform[\"train\"], \"train.txt\")\n",
    "print(len(train_data_set))\n",
    "print('sss')\n",
    "# for index in random.sample(range(0, len(train_data_set)), k=5):\n",
    "#     img, target = train_data_set[index]\n",
    "#     img = ts.ToPILImage()(img)\n",
    "#     plot_img = draw_objs(img,\n",
    "#                          target[\"boxes\"].numpy(),\n",
    "#                          target[\"labels\"].numpy(),\n",
    "#                          np.ones(target[\"labels\"].shape[0]),\n",
    "#                          category_index=category_index,\n",
    "#                          box_thresh=0.5,\n",
    "#                          line_thickness=3,\n",
    "#                          font='arial.ttf',\n",
    "#                          font_size=20)\n",
    "#     plt.imshow(plot_img)\n",
    "#     plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('rcnn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0477a58661eae540eafd0947b12d638aff1bda84ff916b5b978ae71ce1f7f0f5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
